# global configuration file

## GOOD MODELS
# lmsys/vicuna-7b-v1.3
# tiiuae/falcon-7b-instruct
# meta-llama/Llama-2-7b-chat-hf

# generation options
model = "meta-llama/Llama-2-7b-chat-hf" # model to use for generation
device = 'cuda' # default device for models and embeddings
context_size = 512 # maximum size of the input context
batch_size = 4 # number of samples to generate in parallel

# embedding specific
embed = 'paraphrase-MiniLM-L6-v2'
